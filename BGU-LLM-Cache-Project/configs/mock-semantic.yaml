
mode: mock

# ollama:
#   base_url: "http://localhost:11434"
#   model: "llama3.2:1b"
#   timeout_s: 120
#   keep_alive: "1m"
#   options:
#     num_ctx: 1024
#     num_predict: 128
cache:
  type: semantic
  eviction: LRU
  max_size: 5
  clean_size: 2
  similarity: distance       
  similarity_threshold: 0.95      # identical ≈ 1.0 ⇒ HIT
paths:
  artifacts_dir: results/mock-semantic
  reset_artifacts: true
run:
  strict_cold: true
  prompts_file: "data/prompts_mixed.txt"
  warm_repeats: 1
  reverse_warm: true
